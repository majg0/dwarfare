So here we are again. I don't know exactly what to do.
We need a milestone. The first milestone which will drive progress.
I will try to outline the ideas I have right now.

# Platform Layer

> DECISION: We support multiple platforms, to increase market share.
> DECISION: There must be a separation between platform layer and game layer, such that the game can be easily ported.
> DECISION: The interface exposed by the platform layer must be stable, so that we can switch game version.
> DECISION: We will at minimum support both x86 and aarch, native Linux, OSX, Windows, and Steam on top of these, all to increase market share.
> DECISION: We will need to cross compile and test the game over each platform layer, to guarantee that everything works.

## Automated Tests

> DECISION: We need rigorous testing in order to ensure a smooth experience across platforms, because we're building something complicated yet want to rest easy.

I investigated QEMU and it only allows user space emulation for different ISAs, not OSs. We would still require virtual machines or actual hardware. I have my computer dual-booted between Linux and Windows, and I own a Mac, so if we just run the test suite on different OSs every now and then, it should be fine.

> DECISION: We temporarily resort to running tests on different platforms manually, until we reach a point where we're actively working on it enough that it makes sense to have a build system hooked up to dedicated OSs.

I went off now looking into a combination of QEMU, OSX-KVM (over QEMU), and Wine, just to double check, to see if it's actually possible to run everything in one test suite. While it sorta kinda coulda worked ish, it wouldn't be reliable, and it'd be sunk cost in the end, because the whole point is to be able to rest easy knowing that stuff is solid. We could however run QEMU on Linux at some point in order to try e.g. aarch, and to ensure that we didn't screw up with dynamically linking to some distribution-specific libraries. PowerPC should also be attempted if the rumours I heard about it being big endian, in contrast to basically everything else, are true.

That could work, but I imagine we will need more advanced tests, like spinning up three game instances and simulating some ticks on each of them with networking in between, and comparing hashes and logs afterwards. I still wonder how that would be accomplished.

I also thought about gathering code coverage, and checked how to do it for zig. There is precedent with `kcov` on Unix platforms, but I'm not convinced that we would benefit from it.

### Test Setup

How do we organize our tests? We already need to run unit tests to verify the pre-existing code already made before the rambling design style, like the bitpacker and ECS.

Additionally, we need to test more complicated things like hotswapping of game code from the platform layer.
The obvious way to run such a test is to build a special game plugin which increments a counter on reload, and have it be loaded and checked from the platform layer.
This tells something about the capabilities:

1. The platform layer needs to be run from tests; arguably, we don't run the whole layer, but just the part we need in the test, in a test block.
2. We need special support from `build.zig` to explain how to build this test, as it depends on a custom little library. Maybe we could generalize it to avoid a special case?
3. We need to access the file system from inside some of our tests, which means that they're probably to be categorized as integration tests rather than unit tests.
4. The custom game lib needs to write to memory which the platform layer will check for correctness in the integration test, meaning the game also needs to know about the platform layer, not only the other way around.

This should serve as a good proof of concept, but I'm wondering whether we really need a dedicated test for this particular combination of things.
Maybe it would make more sense to test features in more isolation, like the file watcher separately from the hotreloading.
The takeaways are basically the same: we need to run tests across layers, we need special build system code, we need file system access, and we need to write to platform-known memory from the game.

This raises the question: how do we organize tests?. I went ahead and researched it just now, and implemented something very similar to what TigerBeetle does:
> DECISION: We declare a `test` meta-target in `build.zig`, which runs the `test:unit` and `test:integration` targets, unless a filter is passed, in which case it only runs unit tests with filenames or test names matching the filter. Integration tests depend on the install target and may access the build artifacts, but unit tests do not. We can add more targets to the `test` meta-target and further refine filtering if specificity and speed is required. We can improve this design when we add more testing capabilities.
Very nice! It pays off to do some research.

I will go to bed now, but there are open tasks ready to be tackled:

- testing the existing platform layer
- splitting up the existing platform executable code
- structuring the file tree
- settle how we could update the platform layer for users; probably we simply use another binary for this, an updater, while they consider the platform layer the main binary, so we name that the name of the game, but let's think about it some more
- mod structure, can the game be just another mod?
- adding benchmarks
- creating a code style guideline to help document how to manage allocating and initializing memory in zig
- should we add some basic CI already?

I'm very happy that we have a basic test setup now!
